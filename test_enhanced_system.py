#!/usr/bin/env python3
"""
<<<<<<< HEAD
Test the enhanced test system
=======
Test script for enhanced autonomous learning system
Verifies all components are working correctly
>>>>>>> d1b3e6353067c4166fd183c12c225678794528f5
"""

import asyncio
import sys
import os
<<<<<<< HEAD

# Add the app directory to the Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

async def test_enhanced_system():
    """Test the enhanced test system"""
    try:
        from app.core.database import init_database
        from app.services.custody_protocol_service import CustodyProtocolService, TestDifficulty
        
        print("ðŸ§ª Testing Enhanced Test System...")
        
        # Initialize database first
        await init_database()
        print("âœ… Database initialized")
        
        # Initialize the custody protocol service
        custody_service = await CustodyProtocolService.initialize()
        print("âœ… Custody Protocol Service initialized")
        
        # Test 1: Administer a custody test
        print("\nðŸ“‹ Testing custody test for imperium...")
        result = await custody_service.administer_custody_test('imperium')
        print(f"âœ… Custody test result: {result.get('success', False)}")
        
        # Test 2: Administer Olympic event
        print("\nðŸ† Testing Olympic event...")
        olympic_result = await custody_service.administer_olympic_event(
            participants=['imperium', 'guardian'],
            difficulty=TestDifficulty.INTERMEDIATE
        )
        print(f"âœ… Olympic event result: {olympic_result.get('success', False)}")
        
        # Test 3: Administer collaborative test
        print("\nðŸ¤ Testing collaborative test...")
        collaborative_result = await custody_service._execute_collaborative_test(
            participants=['imperium', 'guardian'],
            scenario="Design a secure authentication system",
            context={}
        )
        print(f"âœ… Collaborative test result: {collaborative_result.get('success', False)}")
        
        print("\nðŸŽ‰ Enhanced test system is working!")
        
    except Exception as e:
        print(f"âŒ Error testing enhanced system: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_enhanced_system()) 
=======
from datetime import datetime

# Add the app directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'ai-backend-python'))

async def test_enhanced_system():
    """Test the enhanced autonomous learning system"""
    print("ðŸ§ª Testing Enhanced Autonomous Learning System")
    print("=" * 60)
    
    try:
        # Test 1: Import the enhanced service
        print("1ï¸âƒ£ Testing imports...")
        from autonomous_subject_learning_service_enhanced import EnhancedAutonomousSubjectLearningService
        print("   âœ… Enhanced service imported successfully")
        
        # Test 2: Create service instance
        print("2ï¸âƒ£ Testing service initialization...")
        service = EnhancedAutonomousSubjectLearningService()
        print(f"   âœ… Service created with {len(service.autonomous_subjects)} subjects")
        print(f"   âœ… {len(service.ai_subject_mapping)} AI types configured")
        print(f"   âœ… {len(service.proposal_areas)} proposal areas defined")
        
        # Test 3: Test knowledge base building
        print("3ï¸âƒ£ Testing knowledge base building...")
        test_subject = "machine learning"
        knowledge_base = await service.build_enhanced_knowledge_base(test_subject)
        print(f"   âœ… Knowledge base built for '{test_subject}'")
        print(f"   âœ… Learning value: {knowledge_base.get('learning_value', 0):.2f}")
        print(f"   âœ… Code examples: {len(knowledge_base.get('code_examples', []))}")
        print(f"   âœ… Best practices: {len(knowledge_base.get('best_practices', []))}")
        
        # Test 4: Test subject relevance calculation
        print("4ï¸âƒ£ Testing subject relevance calculation...")
        for ai_type in service.ai_subject_mapping.keys():
            relevance = service.calculate_subject_relevance(ai_type, test_subject)
            print(f"   âœ… {ai_type}: {relevance:.2f} relevance to '{test_subject}'")
        
        # Test 5: Test AI subject generation
        print("5ï¸âƒ£ Testing AI subject generation...")
        for ai_type, current_subjects in service.ai_subject_mapping.items():
            new_subjects = await service.generate_ai_suggested_subjects(ai_type, current_subjects)
            print(f"   âœ… {ai_type}: {len(new_subjects)} new subjects suggested")
        
        # Test 6: Test file patterns
        print("6ï¸âƒ£ Testing file analysis patterns...")
        for file_type, patterns in service.file_patterns.items():
            print(f"   âœ… {file_type}: {len(patterns)} patterns configured")
        
        # Test 7: Test service status
        print("7ï¸âƒ£ Testing service status...")
        status = service.get_enhanced_learning_status()
        print(f"   âœ… Learning active: {status['enhanced_autonomous_learning_active']}")
        print(f"   âœ… Proposal generation active: {status['proposal_generation_active']}")
        print(f"   âœ… File analysis active: {status['file_analysis_active']}")
        print(f"   âœ… Subjects available: {status['subjects_available']}")
        
        # Test 8: Test database connectivity (if available)
        print("8ï¸âƒ£ Testing database connectivity...")
        try:
            from app.core.database import get_session
            from app.models.sql_models import OathPaper, AgentMetrics, Proposal
            from sqlalchemy import select
            
            session = get_session()
            async with session as s:
                # Test oath papers
                oath_count = await s.execute(select(OathPaper))
                oath_papers = oath_count.scalars().all()
                print(f"   âœ… Database connected: {len(oath_papers)} oath papers found")
                
                # Test agent metrics
                metrics_count = await s.execute(select(AgentMetrics))
                metrics = metrics_count.scalars().all()
                print(f"   âœ… {len(metrics)} AI agents found in database")
                
                # Test proposals
                proposal_count = await s.execute(select(Proposal))
                proposals = proposal_count.scalars().all()
                print(f"   âœ… {len(proposals)} proposals found in database")
                
        except Exception as e:
            print(f"   âš ï¸ Database test skipped: {e}")
        
        print("\nðŸŽ‰ All tests completed successfully!")
        print("ðŸš€ Enhanced autonomous learning system is ready!")
        
        # Show system summary
        print("\nðŸ“Š System Summary:")
        print(f"   ðŸ“š Subjects available: {len(service.autonomous_subjects)}")
        print(f"   ðŸ¤– AI types: {', '.join(service.ai_subject_mapping.keys())}")
        print(f"   ðŸ“‹ Proposal areas: {len(service.proposal_areas)}")
        print(f"   ðŸ“ File types: {len(service.file_patterns)}")
        print(f"   â° Learning schedule: Every hour + daily cycles")
        print(f"   ðŸ”„ Cross-AI sharing: Active")
        print(f"   ðŸ“ˆ Enhanced growth: Active")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

async def test_monitoring():
    """Test the monitoring system"""
    print("\nðŸ” Testing Monitoring System")
    print("=" * 40)
    
    try:
        # Test comprehensive monitoring
        print("1ï¸âƒ£ Testing comprehensive monitoring...")
        from monitor_enhanced_learning_comprehensive import monitor_enhanced_learning_comprehensive
        await monitor_enhanced_learning_comprehensive()
        print("   âœ… Comprehensive monitoring working")
        
        # Test AI learning summary
        print("2ï¸âƒ£ Testing AI learning summary...")
        from monitor_enhanced_learning_comprehensive import get_ai_learning_summary
        await get_ai_learning_summary()
        print("   âœ… AI learning summary working")
        
        # Test proposal summary
        print("3ï¸âƒ£ Testing proposal summary...")
        from monitor_enhanced_learning_comprehensive import get_proposal_summary
        await get_proposal_summary()
        print("   âœ… Proposal summary working")
        
        print("ðŸŽ‰ All monitoring tests completed!")
        return True
        
    except Exception as e:
        print(f"âŒ Monitoring test failed: {e}")
        return False

if __name__ == "__main__":
    print("ðŸ§ª Enhanced Autonomous Learning System Test Suite")
    print("=" * 70)
    print(f"â° Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run tests
    system_test = asyncio.run(test_enhanced_system())
    monitoring_test = asyncio.run(test_monitoring())
    
    print("\n" + "=" * 70)
    print("ðŸ“‹ Test Results Summary:")
    print(f"   ðŸ§ª System Test: {'âœ… PASSED' if system_test else 'âŒ FAILED'}")
    print(f"   ðŸ” Monitoring Test: {'âœ… PASSED' if monitoring_test else 'âŒ FAILED'}")
    
    if system_test and monitoring_test:
        print("\nðŸŽ‰ All tests passed! Enhanced system is ready to run!")
        print("ðŸš€ You can now start the autonomous learning service.")
    else:
        print("\nâš ï¸ Some tests failed. Please check the errors above.")
    
    print(f"â° Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}") 
>>>>>>> d1b3e6353067c4166fd183c12c225678794528f5
