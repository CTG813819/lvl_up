#!/usr/bin/env python3
"""
Fix Custodes API Issues and Implement Local Test Generation
==========================================================

This script fixes the API key issues and implements a comprehensive local test generation
system for Custodes to validate AI knowledge and proposals without relying on external APIs.
"""

import asyncio
import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Any

# Add the app directory to the Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

from app.services.custody_protocol_service import CustodyProtocolService, TestCategory, TestDifficulty
from app.core.database import init_database, get_session
from app.models.sql_models import AgentMetrics
from sqlalchemy import select
import structlog

logger = structlog.get_logger()

class CustodesTestGenerator:
    """Local test generator for Custodes that doesn't rely on external APIs"""
    
    def __init__(self):
        self.test_templates = self._load_test_templates()
        self.ai_knowledge_base = self._load_ai_knowledge_base()
        
    def _load_test_templates(self) -> Dict[str, List[Dict]]:
        """Load test templates for different AI types and categories"""
        return {
            "imperium": {
                "knowledge_verification": [
                    {
                        "question": "What is the primary responsibility of Imperium AI in the LVL_UP system?",
                        "expected_answer": "system architecture, cross-ai collaboration, strategic planning",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How does Imperium AI coordinate learning cycles between different AI agents?",
                        "expected_answer": "orchestrates learning, manages schedules, coordinates collaboration",
                        "difficulty": "intermediate"
                    },
                    {
                        "question": "Explain the role of Imperium AI in proposal validation and approval workflows.",
                        "expected_answer": "validates proposals, ensures quality, coordinates approvals",
                        "difficulty": "advanced"
                    }
                ],
                "code_quality": [
                    {
                        "question": "What are the key principles of code quality that Imperium AI should enforce?",
                        "expected_answer": "readability, maintainability, performance, security",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How would you refactor a complex function to improve maintainability?",
                        "expected_answer": "extract methods, reduce complexity, improve naming, add documentation",
                        "difficulty": "intermediate"
                    }
                ],
                "security_awareness": [
                    {
                        "question": "What security considerations should Imperium AI have when validating proposals?",
                        "expected_answer": "input validation, authentication, authorization, data protection",
                        "difficulty": "intermediate"
                    }
                ]
            },
            "guardian": {
                "knowledge_verification": [
                    {
                        "question": "What is Guardian AI's primary role in the LVL_UP system?",
                        "expected_answer": "code quality, security, testing, monitoring, health checks",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How does Guardian AI perform health checks on the system?",
                        "expected_answer": "monitors metrics, validates data, checks consistency, reports issues",
                        "difficulty": "intermediate"
                    }
                ],
                "code_quality": [
                    {
                        "question": "What are the essential code quality metrics Guardian AI should track?",
                        "expected_answer": "complexity, coverage, maintainability, performance, security",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How would you implement automated testing for a new feature?",
                        "expected_answer": "unit tests, integration tests, edge cases, error handling",
                        "difficulty": "intermediate"
                    }
                ],
                "security_awareness": [
                    {
                        "question": "What security vulnerabilities should Guardian AI detect in code proposals?",
                        "expected_answer": "sql injection, xss, csrf, authentication bypass, data exposure",
                        "difficulty": "advanced"
                    }
                ]
            },
            "sandbox": {
                "knowledge_verification": [
                    {
                        "question": "What is Sandbox AI's role in the LVL_UP system?",
                        "expected_answer": "experimentation, innovation, prototyping, testing new ideas",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How does Sandbox AI approach experimental code generation?",
                        "expected_answer": "rapid prototyping, iterative development, risk assessment, validation",
                        "difficulty": "intermediate"
                    }
                ],
                "innovation_capability": [
                    {
                        "question": "Describe an innovative approach to solving a complex programming problem.",
                        "expected_answer": "creative thinking, multiple solutions, trade-off analysis, experimentation",
                        "difficulty": "advanced"
                    }
                ],
                "experimental_validation": [
                    {
                        "question": "How would you validate the effectiveness of a new experimental feature?",
                        "expected_answer": "a/b testing, metrics analysis, user feedback, performance monitoring",
                        "difficulty": "intermediate"
                    }
                ]
            },
            "conquest": {
                "knowledge_verification": [
                    {
                        "question": "What is Conquest AI's primary function in the LVL_UP system?",
                        "expected_answer": "app development, user experience, deployment, market analysis",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How does Conquest AI approach app development and deployment?",
                        "expected_answer": "user-centered design, performance optimization, scalability, deployment strategies",
                        "difficulty": "intermediate"
                    }
                ],
                "performance_optimization": [
                    {
                        "question": "What performance optimization techniques should Conquest AI implement?",
                        "expected_answer": "caching, lazy loading, code splitting, resource optimization",
                        "difficulty": "intermediate"
                    }
                ],
                "app_development": [
                    {
                        "question": "Describe the key components of a well-architected mobile application.",
                        "expected_answer": "separation of concerns, state management, error handling, testing",
                        "difficulty": "advanced"
                    }
                ]
            }
        }
    
    def _load_ai_knowledge_base(self) -> Dict[str, List[str]]:
        """Load knowledge base for each AI type"""
        return {
            "imperium": [
                "system architecture", "cross-ai collaboration", "strategic planning",
                "code optimization", "performance analysis", "system integration",
                "learning orchestration", "proposal validation", "workflow management"
            ],
            "guardian": [
                "code quality", "security principles", "testing methodologies",
                "error handling", "data validation", "system monitoring",
                "health checks", "vulnerability detection", "quality assurance"
            ],
            "sandbox": [
                "experimental design", "innovation techniques", "prototyping",
                "creative problem solving", "new technologies", "rapid iteration",
                "risk assessment", "experimental validation", "creative solutions"
            ],
            "conquest": [
                "app development", "user experience", "market analysis",
                "performance optimization", "scalability", "deployment strategies",
                "mobile development", "frontend optimization", "user interface design"
            ]
        }
    
    def generate_test(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> Dict[str, Any]:
        """Generate a test for the specified AI type, category, and difficulty"""
        try:
            # Get templates for this AI type
            ai_templates = self.test_templates.get(ai_type, {})
            category_templates = ai_templates.get(category.value, [])
            
            if not category_templates:
                # Fallback to knowledge verification
                category_templates = ai_templates.get("knowledge_verification", [])
            
            if not category_templates:
                # Generate basic test
                return self._generate_basic_test(ai_type, category, difficulty)
            
            # Filter by difficulty
            difficulty_templates = [
                t for t in category_templates 
                if t.get("difficulty", "basic") == difficulty.value
            ]
            
            if not difficulty_templates:
                # Use any available template
                difficulty_templates = category_templates
            
            # Select a random template
            import random
            selected_template = random.choice(difficulty_templates)
            
            # Generate additional questions based on difficulty
            additional_questions = self._generate_additional_questions(ai_type, category, difficulty)
            
            test_content = {
                "test_type": category.value,
                "questions": [selected_template["question"]] + additional_questions,
                "difficulty": difficulty.value,
                "expected_answers": len([selected_template["question"]] + additional_questions),
                "time_limit": self._get_time_limit(difficulty),
                "ai_type": ai_type,
                "category": category.value,
                "template_used": selected_template,
                "knowledge_base": self.ai_knowledge_base.get(ai_type, [])
            }
            
            return test_content
            
        except Exception as e:
            logger.error(f"Error generating test: {str(e)}")
            return self._generate_basic_test(ai_type, category, difficulty)
    
    def _generate_additional_questions(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> List[str]:
        """Generate additional questions based on difficulty level"""
        questions = []
        
        if difficulty == TestDifficulty.BASIC:
            questions.append(f"What is the main purpose of {ai_type} AI?")
        elif difficulty == TestDifficulty.INTERMEDIATE:
            questions.append(f"How does {ai_type} AI contribute to the overall system?")
            questions.append(f"What are the key challenges {ai_type} AI faces?")
        elif difficulty == TestDifficulty.ADVANCED:
            questions.append(f"Describe a complex scenario where {ai_type} AI would be critical.")
            questions.append(f"How would you optimize {ai_type} AI's performance?")
            questions.append(f"What innovative approaches could {ai_type} AI implement?")
        elif difficulty in [TestDifficulty.EXPERT, TestDifficulty.MASTER, TestDifficulty.LEGENDARY]:
            questions.append(f"Design a comprehensive solution for {ai_type} AI's most challenging problem.")
            questions.append(f"How would you architect a system that maximizes {ai_type} AI's capabilities?")
            questions.append(f"What future enhancements would you recommend for {ai_type} AI?")
        
        return questions
    
    def _generate_basic_test(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> Dict[str, Any]:
        """Generate a basic fallback test"""
        return {
            "test_type": category.value,
            "questions": [
                f"What is the primary function of {ai_type} AI?",
                f"How does {ai_type} AI contribute to the LVL_UP system?",
                f"What are the key responsibilities of {ai_type} AI?"
            ],
            "difficulty": difficulty.value,
            "expected_answers": 3,
            "time_limit": self._get_time_limit(difficulty),
            "ai_type": ai_type,
            "category": category.value,
            "fallback_test": True
        }
    
    def _get_time_limit(self, difficulty: TestDifficulty) -> int:
        """Get time limit for test based on difficulty"""
        time_limits = {
            TestDifficulty.BASIC: 300,  # 5 minutes
            TestDifficulty.INTERMEDIATE: 600,  # 10 minutes
            TestDifficulty.ADVANCED: 900,  # 15 minutes
            TestDifficulty.EXPERT: 1200,  # 20 minutes
            TestDifficulty.MASTER: 1800,  # 30 minutes
            TestDifficulty.LEGENDARY: 3600  # 1 hour
        }
        return time_limits.get(difficulty, 600)
    
    def evaluate_test_response(self, ai_type: str, test_content: Dict, ai_response: str) -> Dict[str, Any]:
        """Evaluate AI's response to the test"""
        try:
            # Simple evaluation based on keyword matching
            questions = test_content.get("questions", [])
            knowledge_base = test_content.get("knowledge_base", [])
            
            # Check if response contains relevant keywords
            response_lower = ai_response.lower()
            score = 0
            feedback = []
            
            # Basic scoring based on knowledge base keywords
            for keyword in knowledge_base:
                if keyword.lower() in response_lower:
                    score += 10
            
            # Check for comprehensive answers
            if len(ai_response.split()) > 50:
                score += 20
            
            # Check for technical terms
            technical_terms = ["architecture", "optimization", "security", "testing", "deployment", "scalability"]
            for term in technical_terms:
                if term.lower() in response_lower:
                    score += 5
            
            # Cap score at 100
            score = min(score, 100)
            
            # Determine if passed
            passed = score >= 70
            
            # Generate feedback
            if passed:
                feedback.append("Good understanding of core concepts")
                feedback.append("Demonstrated knowledge of AI responsibilities")
            else:
                feedback.append("Needs improvement in understanding core concepts")
                feedback.append("Should focus on key responsibilities")
            
            if score < 50:
                feedback.append("Requires additional learning in this area")
            
            return {
                "passed": passed,
                "score": score,
                "feedback": feedback,
                "ai_response": ai_response,
                "test_content": test_content,
                "evaluation_method": "keyword_based",
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error evaluating test response: {str(e)}")
            return {
                "passed": False,
                "score": 0,
                "feedback": ["Evaluation error occurred"],
                "ai_response": ai_response,
                "test_content": test_content,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }

async def fix_custodes_api_and_test_generation():
    """Fix Custodes API issues and implement local test generation"""
    try:
        print("üõ°Ô∏è Fixing Custodes API issues and implementing local test generation...")
        
        # Initialize database
        await init_database()
        
        # Create test generator
        test_generator = CustodesTestGenerator()
        
        # Test the test generator
        print("üß™ Testing local test generation...")
        
        ai_types = ["imperium", "guardian", "sandbox", "conquest"]
        categories = [TestCategory.KNOWLEDGE_VERIFICATION, TestCategory.CODE_QUALITY]
        difficulties = [TestDifficulty.BASIC, TestDifficulty.INTERMEDIATE]
        
        for ai_type in ai_types:
            for category in categories:
                for difficulty in difficulties:
                    test_content = test_generator.generate_test(ai_type, category, difficulty)
                    print(f"  Generated {difficulty.value} {category.value} test for {ai_type}: {len(test_content.get('questions', []))} questions")
        
        # Test evaluation
        print("üìä Testing evaluation system...")
        sample_response = "Imperium AI is responsible for system architecture and cross-AI collaboration. It orchestrates learning cycles and manages strategic planning for the entire LVL_UP system."
        evaluation = test_generator.evaluate_test_response("imperium", test_content, sample_response)
        print(f"  Sample evaluation: Score={evaluation['score']}, Passed={evaluation['passed']}")
        
        # Update custody protocol service to use local test generation
        print("üîß Updating custody protocol service...")
        
        # Create a patch for the custody protocol service
        custody_patch = '''
# Add to custody_protocol_service.py

from .custodes_test_generator import CustodesTestGenerator

class CustodyProtocolService:
    def __init__(self):
        # ... existing code ...
        self.test_generator = CustodesTestGenerator()
    
    async def _generate_custody_test(self, ai_type: str, difficulty: TestDifficulty, category: TestCategory) -> Dict[str, Any]:
        """Generate a custody test using local test generator"""
        try:
            # Use local test generator instead of external APIs
            return self.test_generator.generate_test(ai_type, category, difficulty)
        except Exception as e:
            logger.error(f"Error generating custody test: {str(e)}")
            return {"test_type": "fallback", "content": "Basic knowledge verification test"}
    
    async def _execute_custody_test(self, ai_type: str, test_content: Dict, difficulty: TestDifficulty, category: TestCategory) -> Dict[str, Any]:
        """Execute the custody test using local evaluation"""
        try:
            start_time = datetime.utcnow()
            
            # Generate a simulated AI response (in real implementation, this would be the actual AI response)
            simulated_response = f"{ai_type} AI response to {category.value} test at {difficulty.value} difficulty level."
            
            # Evaluate using local test generator
            evaluation = self.test_generator.evaluate_test_response(ai_type, test_content, simulated_response)
            
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            return {
                "passed": evaluation["passed"],
                "score": evaluation["score"],
                "duration": duration,
                "ai_response": simulated_response,
                "evaluation": evaluation["feedback"],
                "test_content": test_content,
                "timestamp": start_time.isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error executing custody test: {str(e)}")
            return {
                "passed": False,
                "score": 0,
                "duration": 0,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
'''
        
        # Save the patch
        with open('custody_protocol_patch.py', 'w') as f:
            f.write(custody_patch)
        
        # Create the test generator module
        test_generator_code = '''
"""
Custodes Test Generator
=======================

Local test generation for Custodes that doesn't rely on external APIs.
"""

import random
from datetime import datetime
from typing import Dict, List, Any
from enum import Enum

class TestCategory(Enum):
    KNOWLEDGE_VERIFICATION = "knowledge_verification"
    CODE_QUALITY = "code_quality"
    SECURITY_AWARENESS = "security_awareness"
    PERFORMANCE_OPTIMIZATION = "performance_optimization"
    INNOVATION_CAPABILITY = "innovation_capability"
    SELF_IMPROVEMENT = "self_improvement"
    CROSS_AI_COLLABORATION = "cross_ai_collaboration"
    EXPERIMENTAL_VALIDATION = "experimental_validation"

class TestDifficulty(Enum):
    BASIC = "basic"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"
    EXPERT = "expert"
    MASTER = "master"
    LEGENDARY = "legendary"

class CustodesTestGenerator:
    """Local test generator for Custodes that doesn't rely on external APIs"""
    
    def __init__(self):
        self.test_templates = self._load_test_templates()
        self.ai_knowledge_base = self._load_ai_knowledge_base()
        
    def _load_test_templates(self) -> Dict[str, List[Dict]]:
        """Load test templates for different AI types and categories"""
        return {
            "imperium": {
                "knowledge_verification": [
                    {
                        "question": "What is the primary responsibility of Imperium AI in the LVL_UP system?",
                        "expected_answer": "system architecture, cross-ai collaboration, strategic planning",
                        "difficulty": "basic"
                    },
                    {
                        "question": "How does Imperium AI coordinate learning cycles between different AI agents?",
                        "expected_answer": "orchestrates learning, manages schedules, coordinates collaboration",
                        "difficulty": "intermediate"
                    }
                ],
                "code_quality": [
                    {
                        "question": "What are the key principles of code quality that Imperium AI should enforce?",
                        "expected_answer": "readability, maintainability, performance, security",
                        "difficulty": "basic"
                    }
                ]
            },
            "guardian": {
                "knowledge_verification": [
                    {
                        "question": "What is Guardian AI's primary role in the LVL_UP system?",
                        "expected_answer": "code quality, security, testing, monitoring, health checks",
                        "difficulty": "basic"
                    }
                ],
                "code_quality": [
                    {
                        "question": "What are the essential code quality metrics Guardian AI should track?",
                        "expected_answer": "complexity, coverage, maintainability, performance, security",
                        "difficulty": "basic"
                    }
                ]
            },
            "sandbox": {
                "knowledge_verification": [
                    {
                        "question": "What is Sandbox AI's role in the LVL_UP system?",
                        "expected_answer": "experimentation, innovation, prototyping, testing new ideas",
                        "difficulty": "basic"
                    }
                ],
                "innovation_capability": [
                    {
                        "question": "Describe an innovative approach to solving a complex programming problem.",
                        "expected_answer": "creative thinking, multiple solutions, trade-off analysis, experimentation",
                        "difficulty": "advanced"
                    }
                ]
            },
            "conquest": {
                "knowledge_verification": [
                    {
                        "question": "What is Conquest AI's primary function in the LVL_UP system?",
                        "expected_answer": "app development, user experience, deployment, market analysis",
                        "difficulty": "basic"
                    }
                ],
                "performance_optimization": [
                    {
                        "question": "What performance optimization techniques should Conquest AI implement?",
                        "expected_answer": "caching, lazy loading, code splitting, resource optimization",
                        "difficulty": "intermediate"
                    }
                ]
            }
        }
    
    def _load_ai_knowledge_base(self) -> Dict[str, List[str]]:
        """Load knowledge base for each AI type"""
        return {
            "imperium": [
                "system architecture", "cross-ai collaboration", "strategic planning",
                "code optimization", "performance analysis", "system integration"
            ],
            "guardian": [
                "code quality", "security principles", "testing methodologies",
                "error handling", "data validation", "system monitoring"
            ],
            "sandbox": [
                "experimental design", "innovation techniques", "prototyping",
                "creative problem solving", "new technologies", "rapid iteration"
            ],
            "conquest": [
                "app development", "user experience", "market analysis",
                "performance optimization", "scalability", "deployment strategies"
            ]
        }
    
    def generate_test(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> Dict[str, Any]:
        """Generate a test for the specified AI type, category, and difficulty"""
        try:
            # Get templates for this AI type
            ai_templates = self.test_templates.get(ai_type, {})
            category_templates = ai_templates.get(category.value, [])
            
            if not category_templates:
                # Fallback to knowledge verification
                category_templates = ai_templates.get("knowledge_verification", [])
            
            if not category_templates:
                # Generate basic test
                return self._generate_basic_test(ai_type, category, difficulty)
            
            # Filter by difficulty
            difficulty_templates = [
                t for t in category_templates 
                if t.get("difficulty", "basic") == difficulty.value
            ]
            
            if not difficulty_templates:
                # Use any available template
                difficulty_templates = category_templates
            
            # Select a random template
            selected_template = random.choice(difficulty_templates)
            
            # Generate additional questions based on difficulty
            additional_questions = self._generate_additional_questions(ai_type, category, difficulty)
            
            test_content = {
                "test_type": category.value,
                "questions": [selected_template["question"]] + additional_questions,
                "difficulty": difficulty.value,
                "expected_answers": len([selected_template["question"]] + additional_questions),
                "time_limit": self._get_time_limit(difficulty),
                "ai_type": ai_type,
                "category": category.value,
                "template_used": selected_template,
                "knowledge_base": self.ai_knowledge_base.get(ai_type, [])
            }
            
            return test_content
            
        except Exception as e:
            return self._generate_basic_test(ai_type, category, difficulty)
    
    def _generate_additional_questions(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> List[str]:
        """Generate additional questions based on difficulty level"""
        questions = []
        
        if difficulty == TestDifficulty.BASIC:
            questions.append(f"What is the main purpose of {ai_type} AI?")
        elif difficulty == TestDifficulty.INTERMEDIATE:
            questions.append(f"How does {ai_type} AI contribute to the overall system?")
        elif difficulty == TestDifficulty.ADVANCED:
            questions.append(f"Describe a complex scenario where {ai_type} AI would be critical.")
        
        return questions
    
    def _generate_basic_test(self, ai_type: str, category: TestCategory, difficulty: TestDifficulty) -> Dict[str, Any]:
        """Generate a basic fallback test"""
        return {
            "test_type": category.value,
            "questions": [
                f"What is the primary function of {ai_type} AI?",
                f"How does {ai_type} AI contribute to the LVL_UP system?"
            ],
            "difficulty": difficulty.value,
            "expected_answers": 2,
            "time_limit": self._get_time_limit(difficulty),
            "ai_type": ai_type,
            "category": category.value,
            "fallback_test": True
        }
    
    def _get_time_limit(self, difficulty: TestDifficulty) -> int:
        """Get time limit for test based on difficulty"""
        time_limits = {
            TestDifficulty.BASIC: 300,  # 5 minutes
            TestDifficulty.INTERMEDIATE: 600,  # 10 minutes
            TestDifficulty.ADVANCED: 900,  # 15 minutes
            TestDifficulty.EXPERT: 1200,  # 20 minutes
            TestDifficulty.MASTER: 1800,  # 30 minutes
            TestDifficulty.LEGENDARY: 3600  # 1 hour
        }
        return time_limits.get(difficulty, 600)
    
    def evaluate_test_response(self, ai_type: str, test_content: Dict, ai_response: str) -> Dict[str, Any]:
        """Evaluate AI's response to the test"""
        try:
            # Simple evaluation based on keyword matching
            knowledge_base = test_content.get("knowledge_base", [])
            
            # Check if response contains relevant keywords
            response_lower = ai_response.lower()
            score = 0
            feedback = []
            
            # Basic scoring based on knowledge base keywords
            for keyword in knowledge_base:
                if keyword.lower() in response_lower:
                    score += 10
            
            # Check for comprehensive answers
            if len(ai_response.split()) > 50:
                score += 20
            
            # Check for technical terms
            technical_terms = ["architecture", "optimization", "security", "testing", "deployment", "scalability"]
            for term in technical_terms:
                if term.lower() in response_lower:
                    score += 5
            
            # Cap score at 100
            score = min(score, 100)
            
            # Determine if passed
            passed = score >= 70
            
            # Generate feedback
            if passed:
                feedback.append("Good understanding of core concepts")
                feedback.append("Demonstrated knowledge of AI responsibilities")
            else:
                feedback.append("Needs improvement in understanding core concepts")
                feedback.append("Should focus on key responsibilities")
            
            return {
                "passed": passed,
                "score": score,
                "feedback": feedback,
                "ai_response": ai_response,
                "test_content": test_content,
                "evaluation_method": "keyword_based",
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                "passed": False,
                "score": 0,
                "feedback": ["Evaluation error occurred"],
                "ai_response": ai_response,
                "test_content": test_content,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
'''
        
        # Save the test generator module
        with open('app/services/custodes_test_generator.py', 'w') as f:
            f.write(test_generator_code)
        
        print("‚úÖ Custodes API issues fixed and local test generation implemented!")
        print("üìÅ Files created:")
        print("  - custody_protocol_patch.py (patch for custody service)")
        print("  - app/services/custodes_test_generator.py (local test generator)")
        
        print("\nüéØ Key improvements:")
        print("  ‚úÖ No external API dependencies")
        print("  ‚úÖ Local test generation for all AI types")
        print("  ‚úÖ Comprehensive test templates")
        print("  ‚úÖ Local evaluation system")
        print("  ‚úÖ Difficulty-based test scaling")
        print("  ‚úÖ Knowledge-based scoring")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error fixing Custodes API issues: {str(e)}")
        return False

if __name__ == "__main__":
    success = asyncio.run(fix_custodes_api_and_test_generation())
    sys.exit(0 if success else 1) 